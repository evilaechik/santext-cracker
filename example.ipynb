{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Constants\n",
    "K = 100  # example \n",
    "VP_SIZE = 25000  # vocabulary size of public set\n",
    "\n",
    "def build_candidate_vocab(corpus_tokens: List[str], vocab_size: int) -> Tuple[List[str], dict]:\n",
    "    from collections import Counter\n",
    "    counter = Counter(corpus_tokens)\n",
    "    most_common = counter.most_common(vocab_size)\n",
    "    vocab = [word for word, _ in most_common]\n",
    "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "    return vocab, word2idx\n",
    "\n",
    "def load_glove_embeddings(glove_path: str, vocab: List[str]) -> np.ndarray:\n",
    "    emb_dim = 300\n",
    "    embeddings = np.zeros((len(vocab), emb_dim), dtype='float32')\n",
    "    vocab_set = set(vocab)\n",
    "    found = 0\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            if word in vocab_set:\n",
    "                idx = vocab.index(word)\n",
    "                vec = np.array([float(x) for x in parts[1:]], dtype='float32')\n",
    "                embeddings[idx] = vec\n",
    "                found += 1\n",
    "    print(f'Found {found} GloVe vectors for vocab words.')\n",
    "    return embeddings\n",
    "\n",
    "def compute_knn(embeddings: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    D, I = index.search(embeddings, k + 1)\n",
    "    return D[:, 1:], I[:, 1:]\n",
    "\n",
    "def estimate_epsilon(D_knn: np.ndarray, freq_max: float) -> float:\n",
    "    def theoretical_self_prob(eps):\n",
    "        Z = np.exp(-0.5 * eps * np.sqrt(D_knn)).sum(axis=1)\n",
    "        return 1.0 / np.mean(Z)\n",
    "\n",
    "    eps_range = np.linspace(0.1, 30.0, 500)\n",
    "    best_eps = min(eps_range, key=lambda e: abs(theoretical_self_prob(e) - freq_max))\n",
    "    return best_eps\n",
    "\n",
    "def build_likelihood_matrix(D_knn: np.ndarray, I_knn: np.ndarray, eps: float) -> sp.csr_matrix:\n",
    "    n = D_knn.shape[0]\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors = I_knn[i]\n",
    "        distances = np.sqrt(D_knn[i])\n",
    "        probs = np.exp(-0.5 * eps * distances)\n",
    "        probs /= probs.sum()\n",
    "        for j, p in zip(neighbors, probs):\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            vals.append(p)\n",
    "\n",
    "    L = sp.coo_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "    return L.tocsr()\n",
    "\n",
    "def decode_indices(obf_indices: List[int], L: sp.csr_matrix) -> List[int]:\n",
    "    decoded = []\n",
    "    for idx in obf_indices:\n",
    "        candidates = L[:, idx].toarray().flatten()\n",
    "        decoded_idx = np.argmax(candidates)\n",
    "        decoded.append(decoded_idx)\n",
    "    return decoded\n",
    "\n",
    "def santext_attack_pipeline(corpus_tokens: List[str], glove_path: str, obf_indices: List[int], freq_max: float):\n",
    "    vocab, word2idx = build_candidate_vocab(corpus_tokens, VP_SIZE)\n",
    "    embeddings = load_glove_embeddings(glove_path, vocab)\n",
    "    D_knn, I_knn = compute_knn(embeddings, K)\n",
    "    eps_hat = estimate_epsilon(D_knn, freq_max)\n",
    "    print(f\"Estimated epsilon: {eps_hat:.3f}\")\n",
    "    L = build_likelihood_matrix(D_knn, I_knn, eps_hat)\n",
    "    decoded = decode_indices(obf_indices, L)\n",
    "    decoded_words = [vocab[i] for i in decoded]\n",
    "    return decoded_words, eps_hat\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
